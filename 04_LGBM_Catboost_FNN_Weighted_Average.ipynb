{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_LGBM_Catboost_FNN_Weighted_Average.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2Vfyl1fiCQWSY8VSt9pFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Wadhwani/ME498-Project/blob/main/04_LGBM_Catboost_FNN_Weighted_Average.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-2mszKntVyM"
      },
      "source": [
        "This is an ensemble of three models:\n",
        "- LightGBM\n",
        "- Feedforward Neural Network\n",
        "- Catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp4HpURW-luP"
      },
      "source": [
        "The models are trained indivdually, and then the coefficients for a weighted average:\n",
        "\n",
        "$Y_{true} = aY_{FNN} +  bY_{Catboost} +  cY_{LightGBM}$\n",
        "\n",
        "Are created by simple Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0xjAzEzqj07",
        "outputId": "71ece3d5-e040-4fae-cbef-c860e53dadb6"
      },
      "source": [
        "# Open drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8bs56jttJji",
        "outputId": "c83a9375-a19f-4fb7-b275-8a18c463b8e3"
      },
      "source": [
        "# Installing catboost\n",
        "!pip install catboost"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVytCjNkm2t_",
        "outputId": "07c23be1-25b3-4d3e-8ca8-7b153bc49cf7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "import tensorflow as tfb\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import r2_score, mean_squared_log_error, mean_squared_error\n",
        "\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "print(\"TensorFlow version: \",tf.__version__)  #print the version of tensorflow"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnLpoeywRCCQ",
        "outputId": "f02f4b81-a139-46c6-b337-de3a5805ee26"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxljGaK-9rI5"
      },
      "source": [
        "# To build a Feedforward Neural Network\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrQfhSxsqh7n",
        "outputId": "eedd2994-dda5-473f-9a74-f4d9e673d7c6"
      },
      "source": [
        "# Reading training and testing dataset files\n",
        "path = \"drive/My Drive/ASHRAE_DATA/\"\n",
        "x_train = pd.read_csv(path + \"x_train.csv\", index_col=0)\n",
        "x_test = pd.read_csv(path + \"x_test.csv\", index_col=0)\n",
        "y_train = pd.read_csv(path + \"y_train.csv\", index_col=0)\n",
        "y_test = pd.read_csv(path + \"y_test.csv\", index_col=0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsfjaaCiqqq9",
        "outputId": "8e09a9b5-aee1-48cc-c30d-73d0a5b1fbc1"
      },
      "source": [
        "x_train.info()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1024067 entries, 13765924 to 158735\n",
            "Data columns (total 18 columns):\n",
            " #   Column              Non-Null Count    Dtype  \n",
            "---  ------              --------------    -----  \n",
            " 0   site_id             1024067 non-null  int64  \n",
            " 1   air_temperature     1024067 non-null  float64\n",
            " 2   dew_temperature     1024067 non-null  float64\n",
            " 3   sea_level_pressure  1024067 non-null  float64\n",
            " 4   wind_direction      1024067 non-null  float64\n",
            " 5   wind_speed          1024067 non-null  float64\n",
            " 6   building_id         1024067 non-null  int64  \n",
            " 7   primary_use         1024067 non-null  int64  \n",
            " 8   square_feet         1024067 non-null  int64  \n",
            " 9   year_built          1024067 non-null  float64\n",
            " 10  floor_count         1024067 non-null  float64\n",
            " 11  Year                1024067 non-null  int64  \n",
            " 12  Month               1024067 non-null  int64  \n",
            " 13  Day_of_Month        1024067 non-null  int64  \n",
            " 14  Day_of_Year         1024067 non-null  int64  \n",
            " 15  Day_of_Week         1024067 non-null  int64  \n",
            " 16  Hour                1024067 non-null  int64  \n",
            " 17  Daycode             1024067 non-null  int64  \n",
            "dtypes: float64(7), int64(11)\n",
            "memory usage: 148.4 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej18aLTWAwKw"
      },
      "source": [
        "# Scaling data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "x_scaler = StandardScaler() \n",
        "y_scaler = StandardScaler()\n",
        "\n",
        "x_train_scaled = x_scaler.fit_transform(x_train)\n",
        "y_train_scaled = y_scaler.fit_transform(y_train)\n",
        "\n",
        "x_test_scaled = x_scaler.fit_transform(x_test)\n",
        "y_test_scaled = y_scaler.transform(y_test)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVTl4_VBEZaU"
      },
      "source": [
        "## Feedforward Neural Network Training\n",
        "\n",
        "I have included Dropouts to prevent overfitting. I have also included a reduce learning rate feature to make the model decrease learning rate if the MSE does not decrease. I have also included Early Stopping to halt training when the change in MSE is not large.\n",
        "\n",
        "For hyperparameters, I found that larger batch sizes were necessary for good accuracy. I tested batch sizes of [32, 64, 128, 256, 1024, 2048, 4096] and found 1024 to be the perfect balance between speed and accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj7lc_uJqsyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1784e320-1d0a-42b0-e597-0957384146f2"
      },
      "source": [
        "# Creating Feedforward Neural Network Model\n",
        "fnn_model = Sequential()\n",
        "fnn_model.add(Dense(256, input_shape=(18, ), activation='relu', name='inputs'))\n",
        "fnn_model.add(Dense(128, activation='relu', name='dense_4'))\n",
        "fnn_model.add(Dropout(0.3))\n",
        "fnn_model.add(Dense(64, activation='relu', name='dense_5'))\n",
        "fnn_model.add(Dropout(0.3))\n",
        "fnn_model.add(Dense(32, activation='relu', name='dense_6'))\n",
        "fnn_model.add(Dropout(0.3))\n",
        "fnn_model.add(Dense(16, activation='relu', name='dense_7'))\n",
        "fnn_model.add(Dense(1, activation='linear', name='dense_output'))\n",
        "\n",
        "fnn_model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (Dense)               (None, 256)               4864      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_output (Dense)         (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 48,641\n",
            "Trainable params: 48,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyUnpXuw-ciP",
        "outputId": "762499f1-b48c-435e-945e-3970b4cdf168"
      },
      "source": [
        "opt = keras.optimizers.Adam(learning_rate = 0.01)\n",
        "fnn_model.compile(loss='mse', optimizer=opt, metrics=['mse', 'mae'])\n",
        "\n",
        "#Reduce Learning rate on Plateau \n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose = 1)\n",
        "\n",
        "#Earlystopping callback\n",
        "early_stop = EarlyStopping(monitor ='val_loss', min_delta= 1e-3, patience = 50, verbose = 1, restore_best_weights=True)\n",
        "\n",
        "start_time = time.time()\n",
        "history = fnn_model.fit(x_train_scaled, y_train_scaled, callbacks = [early_stop, reduce_lr], \n",
        "                    validation_data=(x_test_scaled, y_test_scaled), epochs=400, batch_size=1024, verbose=1)\n",
        "end_time = time.time()\n",
        "print(\"Training Time: \", end_time-start_time, \" seconds\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.3088 - val_loss: 0.1886 - val_mse: 0.1886 - val_mae: 0.2398\n",
            "Epoch 2/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1597 - mse: 0.1597 - mae: 0.2255 - val_loss: 0.1306 - val_mse: 0.1306 - val_mae: 0.1927\n",
            "Epoch 3/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1473 - mse: 0.1473 - mae: 0.2146 - val_loss: 0.1289 - val_mse: 0.1289 - val_mae: 0.2173\n",
            "Epoch 4/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1405 - mse: 0.1405 - mae: 0.2108 - val_loss: 0.1354 - val_mse: 0.1354 - val_mae: 0.2016\n",
            "Epoch 5/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1341 - mse: 0.1341 - mae: 0.2042 - val_loss: 0.1811 - val_mse: 0.1811 - val_mae: 0.2217\n",
            "Epoch 6/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1555 - mse: 0.1555 - mae: 0.2289 - val_loss: 0.1520 - val_mse: 0.1520 - val_mae: 0.2481\n",
            "Epoch 7/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1282 - mse: 0.1282 - mae: 0.2026 - val_loss: 0.1165 - val_mse: 0.1165 - val_mae: 0.1982\n",
            "Epoch 8/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1143 - mse: 0.1143 - mae: 0.1932 - val_loss: 0.1205 - val_mse: 0.1205 - val_mae: 0.2057\n",
            "Epoch 9/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1116 - mse: 0.1116 - mae: 0.1919 - val_loss: 0.0998 - val_mse: 0.0998 - val_mae: 0.1968\n",
            "Epoch 10/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1188 - mse: 0.1188 - mae: 0.1934 - val_loss: 0.1238 - val_mse: 0.1238 - val_mae: 0.2027\n",
            "Epoch 11/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1377 - mse: 0.1377 - mae: 0.2098 - val_loss: 0.1274 - val_mse: 0.1274 - val_mae: 0.2195\n",
            "Epoch 12/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1271 - mse: 0.1271 - mae: 0.2195 - val_loss: 0.1119 - val_mse: 0.1119 - val_mae: 0.1978\n",
            "Epoch 13/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1131 - mse: 0.1131 - mae: 0.2032 - val_loss: 0.1405 - val_mse: 0.1405 - val_mae: 0.2087\n",
            "Epoch 14/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1163 - mse: 0.1163 - mae: 0.1990 - val_loss: 0.1226 - val_mse: 0.1226 - val_mae: 0.1904\n",
            "Epoch 15/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1207 - mse: 0.1207 - mae: 0.1998 - val_loss: 0.1118 - val_mse: 0.1118 - val_mae: 0.2031\n",
            "Epoch 16/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1114 - mse: 0.1114 - mae: 0.1993 - val_loss: 0.1283 - val_mse: 0.1283 - val_mae: 0.1897\n",
            "Epoch 17/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1337 - mse: 0.1337 - mae: 0.2111 - val_loss: 0.1142 - val_mse: 0.1142 - val_mae: 0.1931\n",
            "Epoch 18/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1214 - mse: 0.1214 - mae: 0.2078 - val_loss: 0.1631 - val_mse: 0.1631 - val_mae: 0.2367\n",
            "Epoch 19/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1511 - mse: 0.1511 - mae: 0.2311 - val_loss: 0.1070 - val_mse: 0.1070 - val_mae: 0.2083\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
            "Epoch 20/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.1070 - mse: 0.1070 - mae: 0.2039 - val_loss: 0.0981 - val_mse: 0.0981 - val_mae: 0.1816\n",
            "Epoch 21/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0963 - mse: 0.0963 - mae: 0.1865 - val_loss: 0.0991 - val_mse: 0.0991 - val_mae: 0.1758\n",
            "Epoch 22/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0934 - mse: 0.0934 - mae: 0.1822 - val_loss: 0.0969 - val_mse: 0.0969 - val_mae: 0.1745\n",
            "Epoch 23/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0894 - mse: 0.0894 - mae: 0.1776 - val_loss: 0.0927 - val_mse: 0.0927 - val_mae: 0.1743\n",
            "Epoch 24/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0875 - mse: 0.0875 - mae: 0.1741 - val_loss: 0.0929 - val_mse: 0.0929 - val_mae: 0.1659\n",
            "Epoch 25/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0862 - mse: 0.0862 - mae: 0.1715 - val_loss: 0.0909 - val_mse: 0.0909 - val_mae: 0.1681\n",
            "Epoch 26/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0849 - mse: 0.0849 - mae: 0.1696 - val_loss: 0.0885 - val_mse: 0.0885 - val_mae: 0.1658\n",
            "Epoch 27/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0853 - mse: 0.0853 - mae: 0.1680 - val_loss: 0.0846 - val_mse: 0.0846 - val_mae: 0.1581\n",
            "Epoch 28/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0815 - mse: 0.0815 - mae: 0.1640 - val_loss: 0.0872 - val_mse: 0.0872 - val_mae: 0.1635\n",
            "Epoch 29/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0804 - mse: 0.0804 - mae: 0.1634 - val_loss: 0.0855 - val_mse: 0.0855 - val_mae: 0.1511\n",
            "Epoch 30/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0774 - mse: 0.0774 - mae: 0.1587 - val_loss: 0.0799 - val_mse: 0.0799 - val_mae: 0.1564\n",
            "Epoch 31/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0777 - mse: 0.0777 - mae: 0.1570 - val_loss: 0.0829 - val_mse: 0.0829 - val_mae: 0.1541\n",
            "Epoch 32/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0754 - mse: 0.0754 - mae: 0.1545 - val_loss: 0.0826 - val_mse: 0.0826 - val_mae: 0.1491\n",
            "Epoch 33/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0733 - mse: 0.0733 - mae: 0.1517 - val_loss: 0.0770 - val_mse: 0.0770 - val_mae: 0.1479\n",
            "Epoch 34/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0735 - mse: 0.0735 - mae: 0.1522 - val_loss: 0.0850 - val_mse: 0.0850 - val_mae: 0.1543\n",
            "Epoch 35/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0727 - mse: 0.0727 - mae: 0.1508 - val_loss: 0.0845 - val_mse: 0.0845 - val_mae: 0.1474\n",
            "Epoch 36/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0728 - mse: 0.0728 - mae: 0.1497 - val_loss: 0.0783 - val_mse: 0.0783 - val_mae: 0.1445\n",
            "Epoch 37/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0713 - mse: 0.0713 - mae: 0.1479 - val_loss: 0.0800 - val_mse: 0.0800 - val_mae: 0.1429\n",
            "Epoch 38/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0710 - mse: 0.0710 - mae: 0.1484 - val_loss: 0.0779 - val_mse: 0.0779 - val_mae: 0.1463\n",
            "Epoch 39/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0697 - mse: 0.0697 - mae: 0.1456 - val_loss: 0.0750 - val_mse: 0.0750 - val_mae: 0.1388\n",
            "Epoch 40/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0691 - mse: 0.0691 - mae: 0.1460 - val_loss: 0.0753 - val_mse: 0.0753 - val_mae: 0.1441\n",
            "Epoch 41/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0720 - mse: 0.0720 - mae: 0.1489 - val_loss: 0.0730 - val_mse: 0.0730 - val_mae: 0.1389\n",
            "Epoch 42/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0714 - mse: 0.0714 - mae: 0.1473 - val_loss: 0.0794 - val_mse: 0.0794 - val_mae: 0.1449\n",
            "Epoch 43/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0694 - mse: 0.0694 - mae: 0.1453 - val_loss: 0.0756 - val_mse: 0.0756 - val_mae: 0.1426\n",
            "Epoch 44/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0683 - mse: 0.0683 - mae: 0.1436 - val_loss: 0.0776 - val_mse: 0.0776 - val_mae: 0.1439\n",
            "Epoch 45/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0678 - mse: 0.0678 - mae: 0.1436 - val_loss: 0.0800 - val_mse: 0.0800 - val_mae: 0.1397\n",
            "Epoch 46/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0677 - mse: 0.0677 - mae: 0.1422 - val_loss: 0.0764 - val_mse: 0.0764 - val_mae: 0.1402\n",
            "Epoch 47/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0667 - mse: 0.0667 - mae: 0.1419 - val_loss: 0.0730 - val_mse: 0.0730 - val_mae: 0.1400\n",
            "Epoch 48/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0674 - mse: 0.0674 - mae: 0.1424 - val_loss: 0.0730 - val_mse: 0.0730 - val_mae: 0.1403\n",
            "Epoch 49/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0665 - mse: 0.0665 - mae: 0.1419 - val_loss: 0.0719 - val_mse: 0.0719 - val_mae: 0.1368\n",
            "Epoch 50/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0669 - mse: 0.0669 - mae: 0.1414 - val_loss: 0.0723 - val_mse: 0.0723 - val_mae: 0.1348\n",
            "Epoch 51/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0676 - mse: 0.0676 - mae: 0.1424 - val_loss: 0.0748 - val_mse: 0.0748 - val_mae: 0.1412\n",
            "Epoch 52/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0681 - mse: 0.0681 - mae: 0.1421 - val_loss: 0.0744 - val_mse: 0.0744 - val_mae: 0.1395\n",
            "Epoch 53/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0665 - mse: 0.0665 - mae: 0.1408 - val_loss: 0.0808 - val_mse: 0.0808 - val_mae: 0.1409\n",
            "Epoch 54/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0668 - mse: 0.0668 - mae: 0.1418 - val_loss: 0.0688 - val_mse: 0.0688 - val_mae: 0.1334\n",
            "Epoch 55/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0654 - mse: 0.0654 - mae: 0.1404 - val_loss: 0.0731 - val_mse: 0.0731 - val_mae: 0.1386\n",
            "Epoch 56/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0658 - mse: 0.0658 - mae: 0.1406 - val_loss: 0.0706 - val_mse: 0.0706 - val_mae: 0.1367\n",
            "Epoch 57/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0642 - mse: 0.0642 - mae: 0.1384 - val_loss: 0.0735 - val_mse: 0.0735 - val_mae: 0.1414\n",
            "Epoch 58/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0655 - mse: 0.0655 - mae: 0.1385 - val_loss: 0.0684 - val_mse: 0.0684 - val_mae: 0.1342\n",
            "Epoch 59/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0672 - mse: 0.0672 - mae: 0.1398 - val_loss: 0.0722 - val_mse: 0.0722 - val_mae: 0.1374\n",
            "Epoch 60/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0641 - mse: 0.0641 - mae: 0.1386 - val_loss: 0.0697 - val_mse: 0.0697 - val_mae: 0.1306\n",
            "Epoch 61/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0645 - mse: 0.0645 - mae: 0.1390 - val_loss: 0.0686 - val_mse: 0.0686 - val_mae: 0.1334\n",
            "Epoch 62/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0645 - mse: 0.0645 - mae: 0.1390 - val_loss: 0.0681 - val_mse: 0.0681 - val_mae: 0.1308\n",
            "Epoch 63/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0633 - mse: 0.0633 - mae: 0.1373 - val_loss: 0.0710 - val_mse: 0.0710 - val_mae: 0.1345\n",
            "Epoch 64/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0625 - mse: 0.0625 - mae: 0.1374 - val_loss: 0.0695 - val_mse: 0.0695 - val_mae: 0.1384\n",
            "Epoch 65/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0614 - mse: 0.0614 - mae: 0.1369 - val_loss: 0.0657 - val_mse: 0.0657 - val_mae: 0.1341\n",
            "Epoch 66/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0611 - mse: 0.0611 - mae: 0.1363 - val_loss: 0.0743 - val_mse: 0.0743 - val_mae: 0.1388\n",
            "Epoch 67/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0615 - mse: 0.0615 - mae: 0.1380 - val_loss: 0.0690 - val_mse: 0.0690 - val_mae: 0.1360\n",
            "Epoch 68/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0609 - mse: 0.0609 - mae: 0.1370 - val_loss: 0.0676 - val_mse: 0.0676 - val_mae: 0.1322\n",
            "Epoch 69/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0616 - mse: 0.0616 - mae: 0.1350 - val_loss: 0.0683 - val_mse: 0.0683 - val_mae: 0.1327\n",
            "Epoch 70/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0598 - mse: 0.0598 - mae: 0.1339 - val_loss: 0.0672 - val_mse: 0.0672 - val_mae: 0.1332\n",
            "Epoch 71/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0585 - mse: 0.0585 - mae: 0.1339 - val_loss: 0.0694 - val_mse: 0.0694 - val_mae: 0.1326\n",
            "Epoch 72/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0612 - mse: 0.0612 - mae: 0.1361 - val_loss: 0.0726 - val_mse: 0.0726 - val_mae: 0.1331\n",
            "Epoch 73/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0590 - mse: 0.0590 - mae: 0.1338 - val_loss: 0.0673 - val_mse: 0.0673 - val_mae: 0.1300\n",
            "Epoch 74/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0599 - mse: 0.0599 - mae: 0.1363 - val_loss: 0.0655 - val_mse: 0.0655 - val_mae: 0.1323\n",
            "Epoch 75/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0596 - mse: 0.0596 - mae: 0.1350 - val_loss: 0.0664 - val_mse: 0.0664 - val_mae: 0.1300\n",
            "Epoch 76/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0596 - mse: 0.0596 - mae: 0.1348 - val_loss: 0.0632 - val_mse: 0.0632 - val_mae: 0.1288\n",
            "Epoch 77/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0599 - mse: 0.0599 - mae: 0.1336 - val_loss: 0.0653 - val_mse: 0.0653 - val_mae: 0.1300\n",
            "Epoch 78/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0596 - mse: 0.0596 - mae: 0.1332 - val_loss: 0.0667 - val_mse: 0.0667 - val_mae: 0.1286\n",
            "Epoch 79/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0575 - mse: 0.0575 - mae: 0.1310 - val_loss: 0.0666 - val_mse: 0.0666 - val_mae: 0.1306\n",
            "Epoch 80/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0576 - mse: 0.0576 - mae: 0.1327 - val_loss: 0.0616 - val_mse: 0.0616 - val_mae: 0.1253\n",
            "Epoch 81/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0590 - mse: 0.0590 - mae: 0.1329 - val_loss: 0.0661 - val_mse: 0.0661 - val_mae: 0.1280\n",
            "Epoch 82/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0586 - mse: 0.0586 - mae: 0.1326 - val_loss: 0.0663 - val_mse: 0.0663 - val_mae: 0.1271\n",
            "Epoch 83/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0601 - mse: 0.0601 - mae: 0.1346 - val_loss: 0.0636 - val_mse: 0.0636 - val_mae: 0.1310\n",
            "Epoch 84/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0586 - mse: 0.0586 - mae: 0.1332 - val_loss: 0.0654 - val_mse: 0.0654 - val_mae: 0.1280\n",
            "Epoch 85/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0563 - mse: 0.0563 - mae: 0.1300 - val_loss: 0.0639 - val_mse: 0.0639 - val_mae: 0.1279\n",
            "Epoch 86/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0563 - mse: 0.0563 - mae: 0.1307 - val_loss: 0.0643 - val_mse: 0.0643 - val_mae: 0.1281\n",
            "Epoch 87/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0559 - mse: 0.0559 - mae: 0.1300 - val_loss: 0.0637 - val_mse: 0.0637 - val_mae: 0.1312\n",
            "Epoch 88/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0578 - mse: 0.0578 - mae: 0.1322 - val_loss: 0.0651 - val_mse: 0.0651 - val_mae: 0.1316\n",
            "Epoch 89/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0573 - mse: 0.0573 - mae: 0.1318 - val_loss: 0.0654 - val_mse: 0.0654 - val_mae: 0.1247\n",
            "Epoch 90/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0560 - mse: 0.0560 - mae: 0.1315 - val_loss: 0.0645 - val_mse: 0.0645 - val_mae: 0.1300\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
            "Epoch 91/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0554 - mse: 0.0554 - mae: 0.1282 - val_loss: 0.0605 - val_mse: 0.0605 - val_mae: 0.1221\n",
            "Epoch 92/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0528 - mse: 0.0528 - mae: 0.1245 - val_loss: 0.0606 - val_mse: 0.0606 - val_mae: 0.1227\n",
            "Epoch 93/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0519 - mse: 0.0519 - mae: 0.1230 - val_loss: 0.0589 - val_mse: 0.0589 - val_mae: 0.1215\n",
            "Epoch 94/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0509 - mse: 0.0509 - mae: 0.1221 - val_loss: 0.0610 - val_mse: 0.0610 - val_mae: 0.1230\n",
            "Epoch 95/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0516 - mse: 0.0516 - mae: 0.1220 - val_loss: 0.0592 - val_mse: 0.0592 - val_mae: 0.1214\n",
            "Epoch 96/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0500 - mse: 0.0500 - mae: 0.1215 - val_loss: 0.0585 - val_mse: 0.0585 - val_mae: 0.1205\n",
            "Epoch 97/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0509 - mse: 0.0509 - mae: 0.1214 - val_loss: 0.0592 - val_mse: 0.0592 - val_mae: 0.1195\n",
            "Epoch 98/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0513 - mse: 0.0513 - mae: 0.1212 - val_loss: 0.0591 - val_mse: 0.0591 - val_mae: 0.1191\n",
            "Epoch 99/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0506 - mse: 0.0506 - mae: 0.1207 - val_loss: 0.0602 - val_mse: 0.0602 - val_mae: 0.1198\n",
            "Epoch 100/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0510 - mse: 0.0510 - mae: 0.1211 - val_loss: 0.0580 - val_mse: 0.0580 - val_mae: 0.1196\n",
            "Epoch 101/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0501 - mse: 0.0501 - mae: 0.1202 - val_loss: 0.0586 - val_mse: 0.0586 - val_mae: 0.1188\n",
            "Epoch 102/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0500 - mse: 0.0500 - mae: 0.1201 - val_loss: 0.0583 - val_mse: 0.0583 - val_mae: 0.1187\n",
            "Epoch 103/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0501 - mse: 0.0501 - mae: 0.1206 - val_loss: 0.0606 - val_mse: 0.0606 - val_mae: 0.1196\n",
            "Epoch 104/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0500 - mse: 0.0500 - mae: 0.1201 - val_loss: 0.0582 - val_mse: 0.0582 - val_mae: 0.1197\n",
            "Epoch 105/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0489 - mse: 0.0489 - mae: 0.1196 - val_loss: 0.0589 - val_mse: 0.0589 - val_mae: 0.1180\n",
            "Epoch 106/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0514 - mse: 0.0514 - mae: 0.1210 - val_loss: 0.0583 - val_mse: 0.0583 - val_mae: 0.1176\n",
            "Epoch 107/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0500 - mse: 0.0500 - mae: 0.1195 - val_loss: 0.0567 - val_mse: 0.0567 - val_mae: 0.1195\n",
            "Epoch 108/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0499 - mse: 0.0499 - mae: 0.1201 - val_loss: 0.0580 - val_mse: 0.0580 - val_mae: 0.1174\n",
            "Epoch 109/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0487 - mse: 0.0487 - mae: 0.1190 - val_loss: 0.0581 - val_mse: 0.0581 - val_mae: 0.1182\n",
            "Epoch 110/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0498 - mse: 0.0498 - mae: 0.1193 - val_loss: 0.0592 - val_mse: 0.0592 - val_mae: 0.1180\n",
            "Epoch 111/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0495 - mse: 0.0495 - mae: 0.1194 - val_loss: 0.0573 - val_mse: 0.0573 - val_mae: 0.1201\n",
            "Epoch 112/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0495 - mse: 0.0495 - mae: 0.1191 - val_loss: 0.0575 - val_mse: 0.0575 - val_mae: 0.1172\n",
            "Epoch 113/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0497 - mse: 0.0497 - mae: 0.1198 - val_loss: 0.0569 - val_mse: 0.0569 - val_mae: 0.1164\n",
            "Epoch 114/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0493 - mse: 0.0493 - mae: 0.1187 - val_loss: 0.0579 - val_mse: 0.0579 - val_mae: 0.1186\n",
            "Epoch 115/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0493 - mse: 0.0493 - mae: 0.1189 - val_loss: 0.0564 - val_mse: 0.0564 - val_mae: 0.1171\n",
            "Epoch 116/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0495 - mse: 0.0495 - mae: 0.1186 - val_loss: 0.0587 - val_mse: 0.0587 - val_mae: 0.1187\n",
            "Epoch 117/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0500 - mse: 0.0500 - mae: 0.1191 - val_loss: 0.0584 - val_mse: 0.0584 - val_mae: 0.1190\n",
            "Epoch 118/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0493 - mse: 0.0493 - mae: 0.1181 - val_loss: 0.0572 - val_mse: 0.0572 - val_mae: 0.1165\n",
            "Epoch 119/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0494 - mse: 0.0494 - mae: 0.1183 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1175\n",
            "Epoch 120/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0482 - mse: 0.0482 - mae: 0.1185 - val_loss: 0.0594 - val_mse: 0.0594 - val_mae: 0.1203\n",
            "Epoch 121/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0490 - mse: 0.0490 - mae: 0.1190 - val_loss: 0.0579 - val_mse: 0.0579 - val_mae: 0.1165\n",
            "Epoch 122/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0489 - mse: 0.0489 - mae: 0.1183 - val_loss: 0.0574 - val_mse: 0.0574 - val_mae: 0.1178\n",
            "Epoch 123/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0490 - mse: 0.0490 - mae: 0.1182 - val_loss: 0.0595 - val_mse: 0.0595 - val_mae: 0.1185\n",
            "Epoch 124/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0486 - mse: 0.0486 - mae: 0.1180 - val_loss: 0.0594 - val_mse: 0.0594 - val_mae: 0.1179\n",
            "Epoch 125/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0486 - mse: 0.0486 - mae: 0.1178 - val_loss: 0.0585 - val_mse: 0.0585 - val_mae: 0.1208\n",
            "\n",
            "Epoch 00125: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
            "Epoch 126/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0479 - mse: 0.0479 - mae: 0.1173 - val_loss: 0.0573 - val_mse: 0.0573 - val_mae: 0.1171\n",
            "Epoch 127/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0486 - mse: 0.0486 - mae: 0.1170 - val_loss: 0.0567 - val_mse: 0.0567 - val_mae: 0.1159\n",
            "Epoch 128/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0473 - mse: 0.0473 - mae: 0.1166 - val_loss: 0.0566 - val_mse: 0.0566 - val_mae: 0.1167\n",
            "Epoch 129/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0475 - mse: 0.0475 - mae: 0.1167 - val_loss: 0.0576 - val_mse: 0.0576 - val_mae: 0.1164\n",
            "Epoch 130/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0475 - mse: 0.0475 - mae: 0.1166 - val_loss: 0.0577 - val_mse: 0.0577 - val_mae: 0.1165\n",
            "Epoch 131/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0488 - mse: 0.0488 - mae: 0.1167 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1162\n",
            "Epoch 132/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0484 - mse: 0.0484 - mae: 0.1167 - val_loss: 0.0572 - val_mse: 0.0572 - val_mae: 0.1160\n",
            "Epoch 133/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0482 - mse: 0.0482 - mae: 0.1165 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1157\n",
            "Epoch 134/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0481 - mse: 0.0481 - mae: 0.1165 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1159\n",
            "Epoch 135/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0475 - mse: 0.0475 - mae: 0.1161 - val_loss: 0.0574 - val_mse: 0.0574 - val_mae: 0.1154\n",
            "\n",
            "Epoch 00135: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
            "Epoch 136/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0482 - mse: 0.0482 - mae: 0.1162 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1156\n",
            "Epoch 137/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0472 - mse: 0.0472 - mae: 0.1160 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1157\n",
            "Epoch 138/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0479 - mse: 0.0479 - mae: 0.1160 - val_loss: 0.0569 - val_mse: 0.0569 - val_mae: 0.1154\n",
            "Epoch 139/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0478 - mse: 0.0478 - mae: 0.1162 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1156\n",
            "Epoch 140/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0482 - mse: 0.0482 - mae: 0.1162 - val_loss: 0.0574 - val_mse: 0.0574 - val_mae: 0.1158\n",
            "Epoch 141/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0484 - mse: 0.0484 - mae: 0.1161 - val_loss: 0.0573 - val_mse: 0.0573 - val_mae: 0.1159\n",
            "Epoch 142/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0472 - mse: 0.0472 - mae: 0.1156 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1158\n",
            "Epoch 143/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0470 - mse: 0.0470 - mae: 0.1155 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1158\n",
            "Epoch 144/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0475 - mse: 0.0475 - mae: 0.1159 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1157\n",
            "Epoch 145/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0477 - mse: 0.0477 - mae: 0.1159 - val_loss: 0.0572 - val_mse: 0.0572 - val_mae: 0.1155\n",
            "\n",
            "Epoch 00145: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
            "Epoch 146/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0471 - mse: 0.0471 - mae: 0.1158 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1157\n",
            "Epoch 147/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0471 - mse: 0.0471 - mae: 0.1157 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1157\n",
            "Epoch 148/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0468 - mse: 0.0468 - mae: 0.1156 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1157\n",
            "Epoch 149/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0473 - mse: 0.0473 - mae: 0.1160 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1157\n",
            "Epoch 150/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0473 - mse: 0.0473 - mae: 0.1156 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1157\n",
            "Epoch 151/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0465 - mse: 0.0465 - mae: 0.1158 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1158\n",
            "Epoch 152/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0473 - mse: 0.0473 - mae: 0.1159 - val_loss: 0.0572 - val_mse: 0.0572 - val_mae: 0.1157\n",
            "Epoch 153/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0469 - mse: 0.0469 - mae: 0.1156 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1158\n",
            "Epoch 154/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0477 - mse: 0.0477 - mae: 0.1158 - val_loss: 0.0570 - val_mse: 0.0570 - val_mae: 0.1157\n",
            "Epoch 155/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0470 - mse: 0.0470 - mae: 0.1159 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1156\n",
            "\n",
            "Epoch 00155: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
            "Epoch 156/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0469 - mse: 0.0469 - mae: 0.1154 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1157\n",
            "Epoch 157/400\n",
            "1001/1001 [==============================] - 4s 4ms/step - loss: 0.0463 - mse: 0.0463 - mae: 0.1153 - val_loss: 0.0571 - val_mse: 0.0571 - val_mae: 0.1157\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00157: early stopping\n",
            "Training Time:  590.1733565330505  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8NQv0hIFh2Y",
        "outputId": "feef5a46-fc34-4629-f324-3a1e7bbb39e4"
      },
      "source": [
        "# Make predictions\n",
        "start_time = time.time()\n",
        "y_predict_scaled = fnn_model.predict(x_test_scaled)\n",
        "end_time = time.time()\n",
        "print(\"Prediction Time: \", end_time-start_time, \" seconds\")\n",
        "y_predict = y_scaler.inverse_transform(y_predict_scaled)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction Time:  10.483282566070557  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_oJc8WMGQ0o",
        "outputId": "5086d7dc-9925-4c23-d9c6-46b91150770f"
      },
      "source": [
        "# Evaluate accuracy\n",
        "print(\"MSE: \", mean_squared_error(y_test, y_predict))\n",
        "print(\"R^2 Score: \", r2_score(y_test, y_predict))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  4811.329506747989\n",
            "R^2 Score:  0.9427561225784716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlZvUHrgXwLs"
      },
      "source": [
        "fnn_model.save(\"FNNModel.h5\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa66wmV7ZGoC"
      },
      "source": [
        "# Catboost Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYYlnonqZFqt",
        "outputId": "0e38cd56-ee3e-49fd-8199-7c4caa73c05c"
      },
      "source": [
        "from catboost import CatBoostRegressor\n",
        "\n",
        "params = {\n",
        "        'n_estimators': 5000,\n",
        "        'learning_rate': 0.25,\n",
        "        'eval_metric': 'RMSE',\n",
        "        'loss_function': 'RMSE',\n",
        "        'metric_period': 10,\n",
        "        'task_type': 'GPU',\n",
        "        'depth': 12,\n",
        "}\n",
        "\n",
        "cb_model = CatBoostRegressor(**params)\n",
        "start_time = time.time()\n",
        "cb_model.fit(x_train, y_train, eval_set=(x_test, y_test), use_best_model=True, verbose=100, early_stopping_rounds=100)\n",
        "end_time = time.time()\n",
        "print(\"Training Time: \", end_time-start_time, \" seconds\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 229.0094245\ttest: 227.8561283\tbest: 227.8561283 (0)\ttotal: 55.4ms\tremaining: 4m 37s\n",
            "100:\tlearn: 44.2079179\ttest: 45.2581648\tbest: 45.2581648 (100)\ttotal: 2.19s\tremaining: 1m 45s\n",
            "200:\tlearn: 38.1181296\ttest: 40.0920984\tbest: 40.0920984 (200)\ttotal: 4.25s\tremaining: 1m 41s\n",
            "300:\tlearn: 35.2105472\ttest: 38.0893134\tbest: 38.0893134 (300)\ttotal: 6.43s\tremaining: 1m 40s\n",
            "400:\tlearn: 32.8232941\ttest: 36.5448243\tbest: 36.5448243 (400)\ttotal: 8.96s\tremaining: 1m 42s\n",
            "500:\tlearn: 31.2608691\ttest: 35.5294649\tbest: 35.5294560 (499)\ttotal: 11.5s\tremaining: 1m 43s\n",
            "600:\tlearn: 29.5973921\ttest: 34.3762726\tbest: 34.3762726 (600)\ttotal: 14.3s\tremaining: 1m 44s\n",
            "700:\tlearn: 27.7732251\ttest: 33.2109054\tbest: 33.2109054 (700)\ttotal: 17.5s\tremaining: 1m 47s\n",
            "800:\tlearn: 26.4468379\ttest: 32.4726187\tbest: 32.4726187 (800)\ttotal: 20.6s\tremaining: 1m 47s\n",
            "900:\tlearn: 25.1268010\ttest: 31.6605174\tbest: 31.6605174 (900)\ttotal: 23.7s\tremaining: 1m 47s\n",
            "1000:\tlearn: 23.9704538\ttest: 31.1018402\tbest: 31.1018402 (1000)\ttotal: 27s\tremaining: 1m 47s\n",
            "1100:\tlearn: 23.0704272\ttest: 30.7569856\tbest: 30.7569856 (1100)\ttotal: 30.2s\tremaining: 1m 46s\n",
            "1200:\tlearn: 22.4619509\ttest: 30.5404631\tbest: 30.5404631 (1200)\ttotal: 33s\tremaining: 1m 44s\n",
            "1300:\tlearn: 21.9454784\ttest: 30.3308021\tbest: 30.3308021 (1300)\ttotal: 35.6s\tremaining: 1m 41s\n",
            "1400:\tlearn: 21.5360685\ttest: 30.2113855\tbest: 30.2113855 (1400)\ttotal: 38.3s\tremaining: 1m 38s\n",
            "1500:\tlearn: 21.1527792\ttest: 30.0817213\tbest: 30.0805825 (1498)\ttotal: 40.9s\tremaining: 1m 35s\n",
            "1600:\tlearn: 20.8067767\ttest: 29.9602014\tbest: 29.9602014 (1600)\ttotal: 43.5s\tremaining: 1m 32s\n",
            "1700:\tlearn: 20.4405511\ttest: 29.8372445\tbest: 29.8372445 (1699)\ttotal: 46.1s\tremaining: 1m 29s\n",
            "1800:\tlearn: 20.0707503\ttest: 29.7046296\tbest: 29.7046296 (1800)\ttotal: 49s\tremaining: 1m 27s\n",
            "1900:\tlearn: 19.7489908\ttest: 29.5649787\tbest: 29.5649722 (1899)\ttotal: 51.7s\tremaining: 1m 24s\n",
            "2000:\tlearn: 19.5061972\ttest: 29.5128416\tbest: 29.5121290 (1992)\ttotal: 54.3s\tremaining: 1m 21s\n",
            "2100:\tlearn: 19.3088681\ttest: 29.4486807\tbest: 29.4486807 (2100)\ttotal: 56.8s\tremaining: 1m 18s\n",
            "2200:\tlearn: 19.1585837\ttest: 29.4011573\tbest: 29.4011573 (2200)\ttotal: 59.3s\tremaining: 1m 15s\n",
            "2300:\tlearn: 19.0292931\ttest: 29.3787364\tbest: 29.3784945 (2298)\ttotal: 1m 1s\tremaining: 1m 12s\n",
            "2400:\tlearn: 18.9384827\ttest: 29.3550990\tbest: 29.3550990 (2400)\ttotal: 1m 4s\tremaining: 1m 9s\n",
            "2500:\tlearn: 18.8819106\ttest: 29.3334940\tbest: 29.3334280 (2477)\ttotal: 1m 6s\tremaining: 1m 6s\n",
            "2600:\tlearn: 18.7510082\ttest: 29.2982861\tbest: 29.2981584 (2596)\ttotal: 1m 9s\tremaining: 1m 3s\n",
            "2700:\tlearn: 18.4898285\ttest: 29.2127229\tbest: 29.2127229 (2700)\ttotal: 1m 11s\tremaining: 1m 1s\n",
            "2800:\tlearn: 18.2233999\ttest: 29.1401517\tbest: 29.1401517 (2800)\ttotal: 1m 14s\tremaining: 58.6s\n",
            "2900:\tlearn: 18.0060651\ttest: 29.0713659\tbest: 29.0713659 (2900)\ttotal: 1m 17s\tremaining: 56s\n",
            "3000:\tlearn: 17.8973561\ttest: 29.0494877\tbest: 29.0494877 (3000)\ttotal: 1m 19s\tremaining: 53.2s\n",
            "3100:\tlearn: 17.7637564\ttest: 29.0150842\tbest: 29.0148098 (3094)\ttotal: 1m 22s\tremaining: 50.4s\n",
            "3200:\tlearn: 17.6226787\ttest: 28.9779012\tbest: 28.9778322 (3198)\ttotal: 1m 24s\tremaining: 47.6s\n",
            "3300:\tlearn: 17.5467504\ttest: 28.9547264\tbest: 28.9544975 (3289)\ttotal: 1m 27s\tremaining: 44.8s\n",
            "3400:\tlearn: 17.4461598\ttest: 28.9238428\tbest: 28.9238428 (3400)\ttotal: 1m 29s\tremaining: 42.1s\n",
            "3500:\tlearn: 17.3559125\ttest: 28.8883938\tbest: 28.8883938 (3500)\ttotal: 1m 31s\tremaining: 39.4s\n",
            "3600:\tlearn: 17.2823181\ttest: 28.8595255\tbest: 28.8595255 (3600)\ttotal: 1m 34s\tremaining: 36.6s\n",
            "3700:\tlearn: 17.1859632\ttest: 28.8239577\tbest: 28.8230894 (3693)\ttotal: 1m 36s\tremaining: 33.9s\n",
            "3800:\tlearn: 17.1213228\ttest: 28.8117261\tbest: 28.8117261 (3800)\ttotal: 1m 39s\tremaining: 31.2s\n",
            "3900:\tlearn: 17.0587200\ttest: 28.7901871\tbest: 28.7901871 (3900)\ttotal: 1m 41s\tremaining: 28.6s\n",
            "4000:\tlearn: 17.0273695\ttest: 28.7793708\tbest: 28.7793378 (3995)\ttotal: 1m 43s\tremaining: 25.9s\n",
            "4100:\tlearn: 16.9781205\ttest: 28.7642101\tbest: 28.7642101 (4100)\ttotal: 1m 46s\tremaining: 23.3s\n",
            "4200:\tlearn: 16.9313797\ttest: 28.7490083\tbest: 28.7489940 (4199)\ttotal: 1m 48s\tremaining: 20.7s\n",
            "4300:\tlearn: 16.8747341\ttest: 28.7357094\tbest: 28.7357072 (4296)\ttotal: 1m 51s\tremaining: 18.1s\n",
            "4400:\tlearn: 16.8373742\ttest: 28.7280893\tbest: 28.7278861 (4393)\ttotal: 1m 53s\tremaining: 15.4s\n",
            "4500:\tlearn: 16.7975437\ttest: 28.7215882\tbest: 28.7210779 (4489)\ttotal: 1m 55s\tremaining: 12.8s\n",
            "4600:\tlearn: 16.7540082\ttest: 28.7043581\tbest: 28.7043559 (4598)\ttotal: 1m 58s\tremaining: 10.2s\n",
            "4700:\tlearn: 16.7238111\ttest: 28.6967042\tbest: 28.6966180 (4676)\ttotal: 2m\tremaining: 7.66s\n",
            "4800:\tlearn: 16.6672802\ttest: 28.6788486\tbest: 28.6788453 (4799)\ttotal: 2m 2s\tremaining: 5.09s\n",
            "4900:\tlearn: 16.6037437\ttest: 28.6610993\tbest: 28.6610816 (4897)\ttotal: 2m 5s\tremaining: 2.53s\n",
            "4999:\tlearn: 16.5490158\ttest: 28.6437243\tbest: 28.6437165 (4998)\ttotal: 2m 7s\tremaining: 0us\n",
            "bestTest = 28.64371654\n",
            "bestIteration = 4998\n",
            "Shrink model to first 4999 iterations.\n",
            "Training Time:  130.93042969703674  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ_W1IDUZzOQ",
        "outputId": "d0505e45-9032-405b-b2ee-d6b9db282189"
      },
      "source": [
        "start_time = time.time()\n",
        "y_predict = cb_model.predict(x_test)\n",
        "end_time = time.time()\n",
        "print(\"Prediction Time: \", end_time-start_time, \" seconds\")\n",
        "print(\"MSE: \", mean_squared_error(y_test, y_predict))\n",
        "print(\"R^2 Score: \", r2_score(y_test, y_predict))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction Time:  9.947292804718018  seconds\n",
            "MSE:  820.4632600056462\n",
            "R^2 Score:  0.9902383534075645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sAbAHHKv1rP"
      },
      "source": [
        "cb_model.save_model(\"CBMModel.h5\", format='cbm') # Save model"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHpA4cqBbf-2"
      },
      "source": [
        "## Finding Weighted Average\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c5QpZnkbeoy"
      },
      "source": [
        "# Loading LightGBM model:\n",
        "\n",
        "lgbm_model = lgb.Booster(model_file='lightGBMModel.mod')"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLJOt3vlcCmM"
      },
      "source": [
        "# Make predictions on training dataset for each model\n",
        "lgb_pred = lgbm_model.predict(x_train)\n",
        "fnn_pred = y_scaler.inverse_transform(fnn_model.predict(x_train_scaled))\n",
        "cat_pred = cb_model.predict(x_train)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "16YjThXqfFwZ",
        "outputId": "82845671-b375-4fed-fb6f-e310b756b82f"
      },
      "source": [
        "train_avg = pd.DataFrame()\n",
        "train_avg['LightGBM'] = lgb_pred\n",
        "train_avg['FNN'] = fnn_pred\n",
        "train_avg['Catboost'] = cat_pred\n",
        "\n",
        "train_avg.head()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>FNN</th>\n",
              "      <th>Catboost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>270.681268</td>\n",
              "      <td>223.084335</td>\n",
              "      <td>262.350119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>77.138039</td>\n",
              "      <td>54.236919</td>\n",
              "      <td>63.095787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.167184</td>\n",
              "      <td>18.621090</td>\n",
              "      <td>-2.708787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>63.309174</td>\n",
              "      <td>50.970737</td>\n",
              "      <td>46.821493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>134.556585</td>\n",
              "      <td>99.457054</td>\n",
              "      <td>129.633678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     LightGBM         FNN    Catboost\n",
              "0  270.681268  223.084335  262.350119\n",
              "1   77.138039   54.236919   63.095787\n",
              "2   -1.167184   18.621090   -2.708787\n",
              "3   63.309174   50.970737   46.821493\n",
              "4  134.556585   99.457054  129.633678"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkGPSYzigC4v",
        "outputId": "f021c03f-a93b-41c1-d4fb-4e6bd61a2bb9"
      },
      "source": [
        "weighted_average = LinearRegression()\n",
        "\n",
        "weighted_average.fit(train_avg, y_train)\n",
        "print('Coefficients: ', weighted_average.coef_)\n",
        "print('Intercept: %.2f' % weighted_average.intercept_)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients:  [[ 1.01798112 -0.03438864  0.01284118]]\n",
            "Intercept: 0.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "YpXpo-Y3g9ku",
        "outputId": "8f4b8279-45e2-44be-b7ec-c7c94b5aa8f1"
      },
      "source": [
        "# Evaluating Weighted Average Model\n",
        "lgb_pred1 = lgbm_model.predict(x_test)\n",
        "fnn_pred1 = y_scaler.inverse_transform(fnn_model.predict(x_test_scaled))\n",
        "cat_pred1 = cb_model.predict(x_test)\n",
        "\n",
        "test_avg = pd.DataFrame()\n",
        "test_avg['LightGBM'] = lgb_pred1\n",
        "test_avg['FNN'] = fnn_pred1\n",
        "test_avg['Catboost'] = cat_pred1\n",
        "\n",
        "test_avg.head()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LightGBM</th>\n",
              "      <th>FNN</th>\n",
              "      <th>Catboost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>54.867965</td>\n",
              "      <td>69.785774</td>\n",
              "      <td>65.843550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.016331</td>\n",
              "      <td>18.621090</td>\n",
              "      <td>20.584380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>283.091721</td>\n",
              "      <td>301.380249</td>\n",
              "      <td>288.583216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.381409</td>\n",
              "      <td>18.621090</td>\n",
              "      <td>1.938315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.580406</td>\n",
              "      <td>18.621090</td>\n",
              "      <td>2.328023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     LightGBM         FNN    Catboost\n",
              "0   54.867965   69.785774   65.843550\n",
              "1   16.016331   18.621090   20.584380\n",
              "2  283.091721  301.380249  288.583216\n",
              "3    6.381409   18.621090    1.938315\n",
              "4   -0.580406   18.621090    2.328023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-YtaOblhRnQ",
        "outputId": "a2ed0096-6600-456e-bd31-1133ce27773c"
      },
      "source": [
        "y_predict = weighted_average.predict(test_avg)\n",
        "print(\"MSE: \", mean_squared_error(y_test, y_predict))\n",
        "print(\"R^2 Score: \", r2_score(y_test, y_predict))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  474.3699966150309\n",
            "R^2 Score:  0.9943560760283418\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}